# Scraper d'ArrÃªtÃ©s de Paris - Voirie et DÃ©placements

Ce projet scrape automatiquement les arrÃªtÃ©s de la catÃ©gorie "Voirie et dÃ©placements" depuis le [Bulletin Officiel de la Ville de Paris (BOVP)](https://bovp.apps.paris.fr/).

## ğŸ“Š FonctionnalitÃ©s

- **Scraping automatique** : Collecte quotidienne des nouveaux arrÃªtÃ©s via GitHub Actions
- **DÃ©tection des nouveaux arrÃªtÃ©s** : BasÃ©e sur le numÃ©ro unique d'arrÃªtÃ© (ex: 2025 T 17858)
- **MÃ©tadonnÃ©es complÃ¨tes** : Titre, dates, signataires, autoritÃ© responsable
- **Stockage des PDFs** : Upload automatique vers S3
- **Export CSV** : MÃ©tadonnÃ©es exportÃ©es dans `data/arretes.csv`
- **Scraping asynchrone** : ParallÃ©lisation des requÃªtes pour optimiser la vitesse

## ğŸ—ï¸ Architecture

```
scrapearretesparis/
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ daily_scrape.yml          # GitHub Action (exÃ©cution quotidienne)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper.py                # Script principal avec Playwright
â”‚   â”œâ”€â”€ s3_uploader.py            # Gestion upload S3
â”‚   â””â”€â”€ config.py                 # Configuration
â”œâ”€â”€ data/
â”‚   â””â”€â”€ arretes.csv               # MÃ©tadonnÃ©es des arrÃªtÃ©s
â”œâ”€â”€ requirements.txt              # DÃ©pendances Python
â”œâ”€â”€ .env.example                  # Template des variables d'environnement
â””â”€â”€ README.md
```

## ğŸš€ Installation

### 1. Cloner le repository

```bash
git clone https://github.com/VOTRE_USERNAME/scrapearretesparis.git
cd scrapearretesparis
```

### 2. Installer les dÃ©pendances

**Option A - Avec uv (recommandÃ©, 10-100x plus rapide)** :

```bash
# Installer uv
curl -LsSf https://astral.sh/uv/install.sh | sh

# Installer les dÃ©pendances systÃ¨me (Linux)
sudo apt-get install -y libxml2-dev libxslt-dev

# Installer les dÃ©pendances Python
uv pip install --system -r requirements.txt

# Installer les navigateurs Playwright
playwright install chromium
```

**Option B - Avec pip (mÃ©thode classique)** :

```bash
# Installer les dÃ©pendances systÃ¨me (Linux)
sudo apt-get install -y libxml2-dev libxslt-dev

# Installer les dÃ©pendances Python
pip install -r requirements.txt

# Installer les navigateurs Playwright
playwright install chromium
```

### 3. Configurer les variables d'environnement

Copier `.env.example` vers `.env` et remplir les valeurs :

```bash
cp .env.example .env
```

Ã‰diter `.env` :

```bash
# Configuration S3 / MinIO pour le stockage des PDFs
AWS_ACCESS_KEY_ID=your_access_key_here
AWS_SECRET_ACCESS_KEY=your_secret_key_here
AWS_REGION=us-east-1
S3_BUCKET_NAME=your_bucket_name_here

# Pour MinIO ou autre S3-compatible: spÃ©cifier l'URL
# Exemples: http://localhost:9000 ou https://minio.example.com
# Laisser vide pour AWS S3 standard
S3_ENDPOINT_URL=https://minio.example.com

# Configuration du scraper
SCRAPE_DELAY_SECONDS=2
MAX_CONCURRENT_PAGES=5
MAX_PAGES_TO_SCRAPE=0  # 0 = toutes les pages
```

### 4. Configurer GitHub Secrets (pour l'automatisation)

**Option A - Secrets au niveau du repository (recommandÃ© pour dÃ©buter)** :

Dans votre repository GitHub, aller dans `Settings > Secrets and variables > Actions > Repository secrets` et ajouter :

- `AWS_ACCESS_KEY_ID`
- `AWS_SECRET_ACCESS_KEY`
- `AWS_REGION`
- `S3_BUCKET_NAME`
- `S3_ENDPOINT_URL` (laisser vide pour AWS S3, ou votre URL MinIO, ex: `https://minio.example.com`)

**Option B - CrÃ©er un Environment (recommandÃ© pour la production)** :

1. Dans votre repository : `Settings > Environments > New environment`
2. Nommez-le `production`
3. Ajoutez les mÃªmes 5 secrets dans cet environnement
4. Dans `.github/workflows/daily_scrape.yml`, dÃ©commentez la ligne `# environment: production`
5. (Optionnel) Configurez des protections : approbation manuelle, restrictions de branches, etc.

**Note pour MinIO** : Le scraper supporte nativement MinIO et autres services compatibles S3. Il suffit de spÃ©cifier votre endpoint dans `S3_ENDPOINT_URL`. Consultez [MINIO_SETUP.md](MINIO_SETUP.md) pour un guide complet de configuration MinIO.

## ğŸ’» Utilisation

> ğŸ“– **Guide complet de test** : Consultez [TESTING.md](TESTING.md) pour un guide dÃ©taillÃ© des diffÃ©rentes mÃ©thodes de test

### ExÃ©cution manuelle

```bash
cd src
python scraper.py
```

### Mode test (DRY_RUN)

Pour tester le scraper sans uploader vers S3 :

```bash
export DRY_RUN=true
export MAX_PAGES_TO_SCRAPE=1  # Limiter Ã  1 page pour les tests
cd src
python scraper.py
```

Le mode DRY_RUN :
- Ne nÃ©cessite pas de credentials S3
- Simule l'upload des PDFs
- Enregistre quand mÃªme les mÃ©tadonnÃ©es dans le CSV
- Affiche `[DRY_RUN]` dans les logs

### ExÃ©cution automatique

Le GitHub Action s'exÃ©cute automatiquement tous les jours Ã  6h du matin (heure de Paris).

### Test avec GitHub Actions (mode dry-run)

Pour tester le scraper sans uploader vers S3 :

1. Allez dans l'onglet **Actions** de votre repo GitHub
2. SÃ©lectionnez **"Test Scraper (Dry Run)"** dans la liste des workflows
3. Cliquez sur **"Run workflow"**
4. Configurez les paramÃ¨tres :
   - **max_pages** : `1` (nombre de pages Ã  scraper)
   - **dry_run** : `true` (pas d'upload S3 rÃ©el)
   - **max_concurrent** : `3` (pages en parallÃ¨le)
5. Cliquez sur **"Run workflow"** (bouton vert)

Le workflow va :
- âœ… Scraper 1 page de rÃ©sultats
- âœ… Simuler l'upload des PDFs (pas d'upload rÃ©el)
- âœ… Afficher un rÃ©sumÃ© dans l'interface GitHub
- âœ… Uploader les logs et le CSV comme artefacts (tÃ©lÃ©chargeables pendant 7 jours)

### Lancement manuel du scraping complet

Vous pouvez aussi lancer manuellement le scraping complet depuis l'interface GitHub :
1. Aller dans l'onglet `Actions`
2. SÃ©lectionner `Daily Scrape of Paris ArrÃªtÃ©s`
3. Cliquer sur `Run workflow`

## ğŸ“ Structure des donnÃ©es

### CSV (`data/arretes.csv`)

Colonnes :
- `numero_arrete` : NumÃ©ro unique (ex: "2025 T 17858")
- `titre` : Titre complet de l'arrÃªtÃ©
- `autorite_responsable` : Ex: "Direction de la Voirie et des DÃ©placements"
- `signataire` : Nom du signataire
- `date_publication` : Date de publication au BOVP
- `date_signature` : Date de signature de l'arrÃªtÃ©
- `poids_pdf_ko` : Taille du PDF en Ko
- `explnum_id` : ID interne du document dans le systÃ¨me BOVP
- `pdf_s3_url` : URL S3 du PDF (`s3://bucket/arretes/2025/2025_T_17858_abc12345.pdf`)
- `date_scrape` : Date et heure du scraping (ISO 8601)

### S3

Les PDFs sont organisÃ©s par annÃ©e :
```
s3://your-bucket/arretes/
â”œâ”€â”€ 2025/
â”‚   â”œâ”€â”€ 2025_T_17858_a1b2c3d4.pdf
â”‚   â”œâ”€â”€ 2025_T_17859_b2c3d4e5.pdf
â”‚   â””â”€â”€ ...
â”œâ”€â”€ 2024/
â”‚   â””â”€â”€ ...
```

Le hash MD5 (8 premiers caractÃ¨res) est ajoutÃ© au nom de fichier pour Ã©viter les duplicatas.

## âš™ï¸ Configuration avancÃ©e

### Limiter le scraping

Pour tester ou limiter le nombre de pages scrapÃ©es :

```bash
export MAX_PAGES_TO_SCRAPE=5  # Scraper seulement les 5 premiÃ¨res pages
python src/scraper.py
```

### Ajuster la vitesse

Le site BOVP est lent. Les dÃ©lais par dÃ©faut sont :

- `SCRAPE_DELAY_SECONDS=2` : DÃ©lai entre chaque requÃªte
- `MAX_CONCURRENT_PAGES=5` : Nombre de pages ouvertes en parallÃ¨le

Vous pouvez augmenter ces valeurs si vous rencontrez des timeouts.

### Logs

Les logs sont disponibles :
- En console pendant l'exÃ©cution
- Dans `src/scraper.log`
- Dans les artifacts GitHub Actions (conservÃ©s 30 jours)

## ğŸ”§ DÃ©pendances

- **Python 3.11+**
- **uv** : Gestionnaire de paquets ultra-rapide (recommandÃ©) - [Pourquoi uv ?](docs/UV.md)
- **Playwright** : Navigateur headless pour JavaScript
- **BeautifulSoup4** : Parsing HTML
- **Pandas** : Gestion CSV
- **Boto3** : Upload S3
- **python-dotenv** : Variables d'environnement

## ğŸ“Š Statistiques

Au 24 octobre 2025, le site BOVP contient environ **22 420 arrÃªtÃ©s** dans la catÃ©gorie "Voirie et dÃ©placements".

## ğŸ› ProblÃ¨mes connus

1. **Site lent** : Le site BOVP peut Ãªtre trÃ¨s lent. Les timeouts sont configurÃ©s Ã  60 secondes.
2. **TÃ©lÃ©chargement PDF** : Certains PDFs peuvent Ãªtre inaccessibles (document retirÃ©, erreur serveur). Dans ce cas, le scraper enregistre `ERROR: PDF non tÃ©lÃ©chargÃ©` dans le CSV.
3. **Rate limiting** : Si trop de requÃªtes sont faites rapidement, le site peut bloquer temporairement. Ajustez `SCRAPE_DELAY_SECONDS`.

## ğŸ“ Licence

Ce projet est sous licence MIT.

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  ouvrir une issue ou une pull request.

## âš ï¸ Avertissement

Ce scraper est conÃ§u pour un usage Ã©ducatif et de recherche. Assurez-vous de respecter les conditions d'utilisation du site BOVP et les lois en vigueur concernant le scraping de donnÃ©es publiques.
