# Guide de Test du Scraper

Ce guide explique comment tester le scraper en toute sÃ©curitÃ© avant de le dÃ©ployer en production.

## ğŸ§ª MÃ©thodes de test disponibles

### 1. Test local avec DRY_RUN (recommandÃ© pour dÃ©buter)

**Avantages** :
- âœ… Pas besoin de credentials S3/MinIO
- âœ… Rapide et sÃ»r
- âœ… Voir les logs en temps rÃ©el

**Comment faire** :

```bash
# 1. Configurer les variables
export DRY_RUN=true
export MAX_PAGES_TO_SCRAPE=1

# 2. Lancer le scraper
cd src
python scraper.py

# 3. VÃ©rifier les rÃ©sultats
cat ../data/arretes.csv
cat scraper.log
```

**Ce qui se passe** :
- Le scraper va scraper 1 page (environ 50 arrÃªtÃ©s)
- Les PDFs seront "tÃ©lÃ©chargÃ©s" mais pas uploadÃ©s
- Les mÃ©tadonnÃ©es seront enregistrÃ©es dans `data/arretes.csv`
- Les logs montreront `[DRY_RUN]` pour les uploads simulÃ©s

---

### 2. Test avec GitHub Actions (sans toucher Ã  MinIO)

**Avantages** :
- âœ… Teste l'environnement de production
- âœ… Pas besoin d'installer localement
- âœ… Logs et artifacts tÃ©lÃ©chargeables
- âœ… Mode DRY_RUN activÃ© par dÃ©faut

**Comment faire** :

#### Ã‰tape 1 : Configurer les secrets (minimum)

Dans `Settings > Secrets > Actions`, ajoutez au minimum :

```
AWS_ACCESS_KEY_ID = dummy_value
AWS_SECRET_ACCESS_KEY = dummy_value
AWS_REGION = us-east-1
S3_BUCKET_NAME = dummy_bucket
S3_ENDPOINT_URL = (laisser vide ou dummy)
```

> âš ï¸ En mode DRY_RUN, ces valeurs ne sont pas utilisÃ©es mais doivent exister

#### Ã‰tape 2 : Lancer le workflow de test

1. Allez sur votre repository GitHub
2. Cliquez sur l'onglet **"Actions"**
3. Dans la liste de gauche, cliquez sur **"Test Scraper (Dry Run)"**
4. Cliquez sur le bouton **"Run workflow"** (Ã  droite)
5. Une popup s'ouvre avec 3 paramÃ¨tres :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Use workflow from: Branch: main        â–¼   â”‚
â”‚                                             â”‚
â”‚ Nombre de pages Ã  scraper (0 = toutes)     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”                                     â”‚
â”‚ â”‚  1  â”‚  â† Commencer par 1 page            â”‚
â”‚ â””â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                                             â”‚
â”‚ Mode DRY_RUN (pas d'upload S3 rÃ©el)        â”‚
â”‚ â˜‘ true     â† Laisser cochÃ©                 â”‚
â”‚                                             â”‚
â”‚ Nombre de pages en parallÃ¨le                â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”                                     â”‚
â”‚ â”‚  3  â”‚  â† 3 est un bon compromis          â”‚
â”‚ â””â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                                             â”‚
â”‚        [Run workflow]  (bouton vert)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

6. Cliquez sur **"Run workflow"** (bouton vert)

#### Ã‰tape 3 : Voir les rÃ©sultats

Le workflow apparaÃ®t dans la liste avec un cercle jaune ğŸŸ¡ (en cours).

Cliquez dessus pour voir :
- Les logs en temps rÃ©el
- La progression du scraping
- Les messages `[DRY_RUN]` confirmant qu'aucun upload n'est fait

#### Ã‰tape 4 : TÃ©lÃ©charger les rÃ©sultats

Une fois terminÃ© (âœ… vert ou âŒ rouge) :

1. Descendez en bas de la page
2. Section **"Artifacts"** :
   - `test-scraper-logs-XXX` : Les logs complets
   - `test-csv-XXX` : Le CSV avec les mÃ©tadonnÃ©es

3. Cliquez sur un artifact pour le tÃ©lÃ©charger (zip)

#### Ã‰tape 5 : Analyser les rÃ©sultats

**Dans les logs** (`test-scraper-logs-XXX.zip`), cherchez :

```
âœ… Bon signe :
- "=== DÃ©marrage du scraper d'arrÃªtÃ©s ==="
- "Lancement du navigateur..."
- "Page 1: X rÃ©sultats trouvÃ©s"
- "[DRY_RUN] Simulation upload: arretes/2025/..."
- "âœ“ ArrÃªtÃ© 2025 T 17858 traitÃ© avec succÃ¨s"

âŒ ProblÃ¨mes potentiels :
- "TimeoutError" â†’ Le site est trop lent, augmenter les timeouts
- "Impossible de tÃ©lÃ©charger le PDF" â†’ ProblÃ¨me avec la mÃ©thode de download
- "Erreur lors du parsing" â†’ Structure HTML changÃ©e
```

**Dans le CSV** (`test-csv-XXX.zip`) :

Ouvrez avec Excel/LibreOffice et vÃ©rifiez :
- Les colonnes sont bien remplies
- Les numÃ©ros d'arrÃªtÃ©s sont corrects (ex: "2025 T 17858")
- Les URLs S3 sont au bon format (ex: "s3://dummy_bucket/arretes/2025/...")

---

### 3. Test avec upload S3/MinIO rÃ©el (avant production)

**Quand utiliser** :
- AprÃ¨s validation du test DRY_RUN
- Pour tester la connexion S3/MinIO
- Pour vÃ©rifier les permissions

**Comment faire** :

#### Via GitHub Actions

1. Configurez **vos vrais secrets** S3/MinIO
2. Lancez le workflow **"Test Scraper (Dry Run)"**
3. **DÃ©cochez** "Mode DRY_RUN" âŒ
4. Gardez **max_pages = 1** (pour ne scraper qu'une page)
5. Lancez

Le scraper va :
- Scraper 1 page rÃ©ellement
- TÃ©lÃ©charger les PDFs
- Les uploader vers votre S3/MinIO
- Enregistrer les mÃ©tadonnÃ©es dans le CSV

#### Via local

```bash
# 1. Configurer avec VOS vrais credentials
cp .env.example .env
nano .env  # Remplir avec vos vrais credentials

# Exemple pour MinIO :
# AWS_ACCESS_KEY_ID=minioadmin
# AWS_SECRET_ACCESS_KEY=minioadmin
# S3_BUCKET_NAME=paris-arretes
# S3_ENDPOINT_URL=https://minio.example.com

# 2. Activer les uploads rÃ©els
export DRY_RUN=false
export MAX_PAGES_TO_SCRAPE=1

# 3. Lancer
cd src
python scraper.py

# 4. VÃ©rifier sur MinIO
# Ouvrez l'interface MinIO et vÃ©rifiez que les PDFs sont bien uploadÃ©s
```

---

## ğŸ› Troubleshooting

### Le workflow Ã©choue avec "Configuration invalide"

**Cause** : Les secrets GitHub ne sont pas configurÃ©s

**Solution** :
- Allez dans `Settings > Secrets > Actions`
- Ajoutez au minimum les 5 secrets (mÃªme avec des valeurs dummy en mode DRY_RUN)

### Erreur Playwright : "Package 'libasound2' has no installation candidate"

**Cause** : IncompatibilitÃ© entre Playwright et Ubuntu 24.04 (ubuntu-latest)

**Solution** : Ce problÃ¨me est dÃ©jÃ  corrigÃ© dans les workflows (on utilise `ubuntu-22.04`)

Si vous rencontrez cette erreur sur vos propres workflows :
```yaml
jobs:
  scrape:
    runs-on: ubuntu-22.04  # â† Changer de ubuntu-latest Ã  ubuntu-22.04
```

**Explication** : Playwright n'est pas encore totalement compatible avec Ubuntu 24.04. Les paquets systÃ¨me `libasound2`, `libffi7` et `libx264-163` ont Ã©tÃ© renommÃ©s ou supprimÃ©s dans cette version.

### TimeoutError sur le tÃ©lÃ©chargement PDF

**Cause** : Le site BOVP est lent ou le PDF n'est pas accessible

**Solution** :
- Augmenter les timeouts dans `scraper.py` (ligne ~XXX)
- VÃ©rifier que l'URL de la visionneuse fonctionne manuellement

### Aucun arrÃªtÃ© trouvÃ©

**Cause** : La structure HTML du site a changÃ©

**Solution** :
- VÃ©rifier l'URL de recherche : https://bovp.apps.paris.fr/index.php?lvl=search_segment&id=121
- VÃ©rifier les sÃ©lecteurs CSS dans `_parse_arrete_from_element()`
- Ouvrir une issue sur GitHub

### CSV vide aprÃ¨s le test

**Cause** : Tous les arrÃªtÃ©s Ã©taient dÃ©jÃ  prÃ©sents

**Solution** :
- Vider le fichier `data/arretes.csv` avant le test
- Ou vÃ©rifier les logs pour voir si des doublons ont Ã©tÃ© dÃ©tectÃ©s

---

## âœ… Checklist avant production

Avant de lancer le scraping complet en production :

- [ ] Test local DRY_RUN rÃ©ussi (1 page)
- [ ] Test GitHub Actions DRY_RUN rÃ©ussi (1 page)
- [ ] Test upload S3/MinIO rÃ©el rÃ©ussi (1 page)
- [ ] PDFs visibles dans le bucket S3/MinIO
- [ ] CSV correctement formatÃ©
- [ ] Secrets GitHub configurÃ©s (production)
- [ ] Environment GitHub crÃ©Ã© (optionnel mais recommandÃ©)
- [ ] DÃ©lai entre requÃªtes adaptÃ© (pas de ban du site)
- [ ] Logs lisibles et sans erreur critique

Une fois tous les tests passÃ©s, vous pouvez :
- Lancer le workflow complet manuellement (`MAX_PAGES_TO_SCRAPE=0`)
- Ou laisser le workflow quotidien se lancer automatiquement

---

## ğŸ“Š Estimation de durÃ©e

Pour rÃ©fÃ©rence, voici les durÃ©es approximatives :

| Pages | ArrÃªtÃ©s | DurÃ©e estimÃ©e | Mode |
|-------|---------|---------------|------|
| 1 | ~50 | 3-5 min | DRY_RUN |
| 1 | ~50 | 5-10 min | Upload S3 |
| 10 | ~500 | 30-60 min | Upload S3 |
| 100 | ~5000 | 5-10h | Upload S3 |
| Toutes (~450) | ~22,400 | 20-40h | Upload S3 |

> âš ï¸ Le scraping complet prendra **trÃ¨s longtemps** en raison du dÃ©lai entre requÃªtes et de la lenteur du site BOVP.

**Recommandation** : Commencez par scraper les 50 premiÃ¨res pages (~2500 arrÃªtÃ©s les plus rÃ©cents) puis augmentez progressivement.

---

## ğŸ”„ Test de mise Ã  jour quotidienne

Pour tester que le systÃ¨me dÃ©tecte bien les nouveaux arrÃªtÃ©s :

1. Lancez un premier scraping (1 page)
2. VÃ©rifiez le CSV : 50 arrÃªtÃ©s
3. Relancez le scraping (mÃªme page)
4. VÃ©rifiez les logs : "ArrÃªtÃ© XXX dÃ©jÃ  prÃ©sent, ignorÃ©"
5. VÃ©rifiez le CSV : toujours 50 arrÃªtÃ©s (pas de doublons)

âœ… Si aucun doublon n'est crÃ©Ã©, le systÃ¨me fonctionne correctement !
